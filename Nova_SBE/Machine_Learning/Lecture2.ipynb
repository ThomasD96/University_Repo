{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, let's make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# allow multiple outputs per cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression using the Normal Equation\n",
    "\n",
    "To find the value of $\\theta$ that minimizes the cost function, there is a *closed-form solution*\n",
    "—in other words, a mathematical equation that gives the result directly. This is called\n",
    "the *Normal Equation*:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\theta} = (X^T\\cdot X)^{-1}\\cdot X^T\\cdot y\n",
    "\\end{equation}\n",
    "\n",
    "</br>\n",
    "Let's generate some data: y=4 + 3x + $\\epsilon$\n",
    "\n",
    "Therefore, our true values of $\\theta$ are as follows:\n",
    "$\\theta_0=4$ and $\\theta_1=3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# rand Return a sample (or samples)  from a uniform distribution over [0, 1)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "#randn Return a sample (or samples) from the “standard normal” distribution\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the link for different options when plotting with pyplot\n",
    "#https://matplotlib.org/3.3.3/api/_as_gen/matplotlib.pyplot.plot.html\n",
    "\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s compute $\\theta$ using the Normal Equation. We will use the *inv()* function from\n",
    "NumPy’s Linear Algebra module (np.linalg) to compute the inverse of a matrix, and\n",
    "the dot() method for matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mp.c_ Translates slice objects to concatenation along the second axis\n",
    "#if you want to get help on some function, just type \"?\" at the end of its name, for exampple np.c_?\n",
    "X_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each instance\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual function that we used to generate the data is $y = 4 + 3x_0$ + Gaussian noise.\n",
    "Let’s see what the equation found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would have hoped for $\\theta_0 = 4$ and $\\theta_1 = 3$ instead of $\\theta_0 = 4.215$ and $\\theta_1 = 2.77$. Close\n",
    "enough, but the noise made it impossible to recover the exact parameters of the original\n",
    "function.\n",
    "\n",
    "Now you can make predictions using $\\hat{\\theta}$. Let's check predictions for points x=0 and points x=3. Do not forget that we need to add the intercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]  # add x0 = 1 to each instance\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s plot this model’s predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_new, y_predict, \"r-\", linewidth=2, label=\"Predictions\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equivalent code using Scikit-Learn looks like this:\n",
    "\n",
    "(Note that Scikit-Learn separates the bias term (intercept_) from the feature weights (coef_).) The parameter fit_interceptbool is by default=True, so we do not need to add the bias term.\n",
    "\n",
    "\n",
    "Let's compare the value of $\\theta$ obtained, as well as the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sklearn=lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_new, y_predict, \"r-\", linewidth=2, label=\"previous predictions\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(X_new, y_predict, \"g-.\", linewidth=2, label=\"sklearn predictions\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {
    "42f4187a-35c2-4c38-97de-821f48cf037a.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAc0AAABaCAYAAADaUQ1SAAAgAElEQVR4Ae29B1MbS7Cw/f7mW9+9xyfZGJschBA555xzzmByzjkjBEoooPx8tSuBJCx8wMYmeLZKxcaZnmeW7ememZ7/xxveHA4HxcXFnJ6evuFSiqIJAoKAICAI/CoC/+9XZfRc+XR0dLC8vPxc2Yt8BQFBQBAQBN4QgTevNCVr0+l0vqEqE0URBAQBQUAQeC4Cb15pSmDtdjsej+e5GIt8BQFBQBAQBN4IgTevNK1WK2VlZVgsljdSZaIYgoAgIAgIAs9F4M0rTZfLJSvNw8PD52Is8hUEBAFBQBB4IwTevNKU6qm9vZ35+fk3UmWiGIKAICAICALPReC3UJqSi9Zmsz0XY5GvICAICAKCwBsh8FsozTdSV6IYgoAgIAgIAs9M4LdQmjqdjvr6eqT+TbEJAoKAICAICALfS+C3UJpms5ns7GwkN63YBAFBQBAQBASB7yXwWyhNKcBBUVER5+fn38tJPCcICAKCgCAgCPBbKE0psIFWq0VSnmK7n4DbZubo8BCD1Qmua04P9zm9vJIfsOjO2Ts4xuxw35+AuCIICAKCwBsn8FsozTdeh09SPI9Nx0RfO4UZKgqa+pgeG6a5uoj4pCyGp6bo62whWxlPaecsQm0+CXKRiCAgCLxCAr+N0pTmaa6vr7/CKvo1IjttZgwWC0udZXxOyGZDY8Np3EIZ9p78tkmkIVTjjdmkVAwg7PVfUyciF0FAEHh5BN6s0ry4uGBubk7+jY6OkpeXR05ODi0tLTQ2NlJeXs7CwsLLq5FnlchKa0Eyxd1eLqbdMeISs9g1SirTSnOeguqR7WeVUGQuCAgCgsBzEnizSlMa9PPp0yfevXvHX3/9xT///CP/pP0///yTsLAwzs7OnpP9i8vbYz4hNyme4S29LNvWUA1Jxe3YAbdxh4z4JCaPTS9ObiGQICAICAK/isCbVZoSwN7eXllBvn//nsCfpDhra2t/FeNXk49x7wux0alsm6QVYex0l6RQ1OkNP2jcHCYiPou1gwO2D9SINWNeTbUKQQUBQeAJCfxUpel2OTCbTFzbnyeogMlkQqlUyhbmjdL8999/ZStzf3//CTG+jaROF3rJq+pGXg/Goae1vICRdY1cOId2l/K8LCqaetjTCGvzbdS4KIUgIAg8lsBPUpouTjbmaG2op7amgry8IsbWTh4r25PcPzMzw99//31raUpWZmFhIW736x8D6rw2sDq/hNb2axolLpcTp1vYmE/yYopEBAFB4FUS+ClK81qzRWNDOwdaM+Bh+0sjn2Mz2DU6fzkkp9NJVlaW3K8pWZtS3+by8vIvl+NJM3Q7OFyZoCAllg+f0tgxi/GsT8pXJCYICAKCwD0EforS9LhcOF1+S85tOCAtOpLe1eeJyCNNNZEUpmRlpqWnIynS17p5HAZGO5to7emjvlDFh/A0ds2vtzyvtR6E3IKAIPB7EvgpSlNC6XE7MGgvOL/U4zCeUaCIoXX+6FkoSxGBSktL+b//+z8mJiaeRYanytTttKLRaOXkVvsqCAtLZeeRStPtsmO6ukLq85V+RqPxO343z5txCJftU1WvSEcQEAReOIGfojTt+iNaKosoqWmkv7+XpqoSosMj6Vo6fjYcBwcHqFQqWUk8mxBPnPFyb/l3KU2LeoWMmAiiomOIiY1HlZpGWmoqqY/4JSXEERUVSVRMBssayQ0vNkFAEBAE3j6Bp1ea15fUZMSRUTeE1eeh3R6u4n//+szYttdCeg6s0rJgarUayep8K9v3Kk2w8aUhj7/e/cGf/3ykdmgJi9XK1dXVg34m0xVnO3Nkx3/kj7/imVOL0bRv5Z0S5RAEBIFvE3hypbn7pY73ESls6/2DU05nmvk7Ion1S/+5b4slrj6EwPcrTfBcn1ObGcef794RFpvOivrx1uLeRCPv/4oVSvMhlSXuEQQEgTdB4ImVppWWrDgSi7qwBeCZacohMrUKg39sUMBVsfu9BH5EaUp5mk+WSY36IEdNisup59L2OCvcoV4mPjyGqVPvSijfWw7xnCAgCAgCr4XA0ypNm5qchHDyupf85XfoKFNGUdT7yqd5+Ev0YvZ+VGlKBdmf6SDinz959+e/FLZPPi4Yu13HQHsPe4brF8NECCIICAKCwM8k8LRK03lJseIzBX2rtzKrl3qJjEljUxtoe95eFjs/QGClzzsQaNfyI1NO7Iw3FfD3n+/4830UvUvPE4TiBzCIRwUBQUAQ+GUEnlZp4mGho4SE7FrOryxo9hbIT1XROStC1j1djXqwGHWcHWxSm5PA//4RTsv4EmqNluvvXCDaY/X2b7774x1hcVlsXFifTlyRkiAgCAgCb4jAEytN8DhMLHwZoKOjg46uXpb2zp89uLc0YlZa0USn0z2o6rRaLScnL9XicrG7MkVPdxft7e20d0i/LvoGRjnSf781bz729m/+8e5PFIWtBIzjehCz+26SlmiTRi2/mM3j4kqvx+7ySC8r58cnmOxOLEYDVvsr6XR32tEbjPJi4B67meOTc+xOJ0a9gVDtJmnkuBRr+fr6Z7rRpcacAYscZ9qNVn2KzmzDZjZyZRUDAF/M+y8E+WECT640f1iixybg8XA7i0Tav/O8xWKhrq6O3Nxcjo6+Dq7gdjmx2ey4bxOBw8NDMjMz5XU3rdbfx+o6kPo3//2TP/58T2nvvLzw9B2cDz6Uoi51dXWRnp5+u/i3x2VHd3HOyfEJOtMPfsDddjRqNVbH3Rr/hohOE9P9rTR1T2B0g0e7SVZGMQdXdk5WxqhtaGdP66tvj53z0zMsobTQN7L42ZecV+f0tdTTPbkuK03t2jDpJc2YnHaWRzpp6BhGawtW/pLSlBpYGRkZbG5u/gQRnWxOD1Db1MOpFGjDqaUiN5vpQyPGo2XqaxtYPLj8CfmKJAWBX0/g1StNp+WCkY4mKiur6B5bwBIQu1xqWUuLT5eUlPC18nNxsDxOfW0d9XXVlNe0c2z0t4glZStFEZKet9m+34L79VX6IznamZD7N//gr7BYhta/z0KUguFXVVXJMX/1eu/anLJULitLgw18Co9mYO37QypaL/dpqymnrucLxgeP+PWw1FdLRdeUvD4objta9QkHpxe3jQPN5hfyixu5lKvbxsJIByXltSwfPt/84qDadFvorSqma3ZPPu22W1CfHHJ2YfTd5mZjtJni5tGg0es3aSwuLhIXF8fa2trNqSf5e7rYR35Vh8874cKoVbN/dMK173/RotmgJL+UrcsfbCg9ibQiEUHgxwi8eqUpFX+hLY//fR/L3FHw1AepdS0tDfa1woTdyVYSVfmsq6+kSYt0l6jIa5sJoikpzoSEBHldzqALb/jA278Zz7s//iBckc+u7vENhunpaSIjI7m8/Nq60G8P8TlCyZrO30B5DE6bbofCtDR65va+8ip8Kx2nbovstDx2bhYNuNbSXJJOWfdCwGMO+sqzaJz298Ff7syQmZrB5N7DXPsBiT35rm5jmJT8Bq58huT15S7F6Up6lgIaNw4N5RkZTB8H/y/cCDM8PExMTAwGg+Hm1I/9dWqpys5gbM/XOPLYmWgtJrWsJ0hxL/dUkN82/WN5iacFgRdA4A0oTQ8TtRlEpVYT+BmQPgoREREhY81en6+SHBVFz8rZbRXMtucTk93I3Sn+IyMjREVFybFZb29+4zsmaf5m9Af++ONPkks7uQqw3v+r6Ha7XW6oNDU1hbx1s7+C6Ixa5HWuQ97xrZMWOotSKOiYfZTClFI8nWlDWdLG9a03105XSQoN08Eu+53xBjJKerzWqE+Ug6lW4tPK0Dy+/fCtwjzymofppiKKO/1l99jVlCSnMH0a+NZ6GKvPoqQ/tDVpNpuJjY1lYGDgkfmHvt16NIcyvYBTeRFW7z2L7YVk32mA6vfGSE6pQBvsOQ6dqDgrCLxgAq9faboNVKZGktoQHIhdsnY+ffqERuNdRNlfB24mG7P5nFKJIWCmxnRzNtHZDV99zKUBRNIKKa9+OTE/gAftHcx2evs3//pAzdDKg56RbpJi/ErLr21tbYV4xk53cQq5HbNYTZfMjA0xOr3Clf1Wk4V4xn9Kv/OF6BgVm1q7/6S85+ZsZ5H+wTGOdb4+SZeVjdkJptcO5TtWeyrJqRvipso91ycUKtOZVwd87YGz5T5SM2uDGmDYLihWxtI4fXAn36c7tBvPmRodYmZ171ZG17WB9aUFFlY2sTqsdJdlUT/q52o9mkGRXsq5vyUgCzTfXUp2/fi9wkmucynOcKiQkm6bkaXJEUan17D6GktW3QkTX76wfxGonL3Ja1YGSMmqQnerDM0056TTtXwalL9Vs0xGYjZ71ofVddDD4kAQeEEEXr3SdOl2UH0Oo2nW+3G8YVtfXy+3qKVBEIGbx3pKbuwnSvsDFYGdrrwEVOV9QRaG9Jzkoo2Pj5cHUgSmE2rfcaVhcX6O+fn5oN/c/CJnxmc1U0KJ+x/n7Ew0F/L3uz/4OzyRid2H9etJq8hIjZVQrlmsGopSE6nvGaK7o4Ou1loi3ofROLHzH7J4L8+25BKf2/KVN+B8c4rGxibykiJJqR2V13Cd66lBERdHSdes/PByZwU5dcO3CsmyP0liZhkaU/D7cbrUgyqjGv2db/tkXRbxBR1BLscHCf2Am9wWDX1tzdRX5PJPWAJLZzeK3Ep/aTJ/RqRzZrHSVZZJw9j2bYr7X5pIr+zCLCmsAHnnOkvIrB0LPHX7jLQzNDQku8+D+pulC55rpvpaaWqoJio8kqEtHZhPqS1IIy5BxeThTd+pPzn1Uh8pmQG8jPtkpKWzIpuefqEs50ukJWSya/af86ci9gSB10PgUUpzdnZWdr2lpaXxM35S/+P4+P0t5FBYLzeH+BgWy/xJcCu4qKhIHi14tzV9tTtOxId/UGYVUllRTnl5BRVlhUR9/EBJz9dRixwOh1zmysrKUNkHnbOcrFBalE9BYSGFt78C8gpLmT8KdB4HPfbNA+nD9r2sHzrF5l4BrBoachP5v//vHcWd8/feFnhBGjEr9ZlJwd/vbpbzZRQf35Ne3o5aGrHlMFCu+ERRUL/i3aduji00ZcSS0Th1c+L2r0l/idUJx5PNRCuLmZrsp6plCOO1HZvN23d6Mt2KsqT91j1r2BwlXpFMU9comoDgENtf6km7EwZSymh/rJbwhDxOQ7Z9PLicDqR35Zs/pzOkInPZTFzoLXiMR6RHhlE7sesrm5PBykxymr1lnmoMds9uDtWRlJxD55fAAXAeRmszKeoNbBTeopJ3FhYWCAsL4/j4zqpDLjsXWgO4rDTnKihqHaK3uYbBpUPsdlvIJeAsh7MoM/zuWZd2h/SkOMqbetnV+AP563dHSFKVcRHcRgkWTBwJAq+AwKOU5uDgIP/zP/8jxyp99+7dk/+V0pbmdz5mW+8t5UN8LicBA/MkRSmNes3Ozv4qqd2xBsIjU1nYP+VcrUatuWBnpptPH8IZCLEKy2OU5leZPcEJyb38Paz/+OMPzs+/f4SqV3QPC53F/BOeyNzR11ZGqOJJSjM6OjpkH/DZXBvvw2KZOvQ2IFymAzJiouhc9Pcth0pTPue8oCgxguJ7+uqkeyzqJZI+fiCtogvdnVG1jst1MlLz2PV10HqcFnbWVjm6DFTuDvorsqib8I5ODZRFvdxNWGQyW7oQX327ga7KHHmJNWmKTchfagqppc1o77hSA/MAMw1ZcaQ1ersaPKYjCtMzWPRZnperA6gKGjH5XKFOi5a1lTUurwI0ueOC8ox0Jg7ury+pq+Hff/+VXenB+fuPFrvL+RgeQ9vkVkhFf3un44KyzHTG932NQo+Tk+11to8v5CkxN/et9FSQ0xLchXJzTfwVBF4TgUcpTcl6kOYwSvMdf8ZPSvtxo/psdBUoSMjv/MqtWlZWJluIwZamh9nWAj6n1WC66dwCNgYrCYvN5dRy2zFzW4c3SrO6uvr23H07TrOWjbVVeUi/NKz/5re6tsGF6W4/3H2pBJ+X5jt+L2tJ9h/ZtNvjxH4Mp37c34/2X+n19PTcozQ9TDZkE5PTzE37xrApuQmTWb8zkvZ8Z4H2libqG9vZufC5Ku1q8hI+UzZy/zxDp2GP1M8faZ4J1ffoYrarkoqu2VsX7d2y6LYnyC6oRRNCsWnW+/kYkcSGNgRTjxOd5oyTk1NOT+/7nXB6rg0ZfCBQjpnGHGKym+X3eX+8jZLWCb/ycV3RUZ5H13xwV0Tg89tfWsivG7hlHHjtZn9mZoaPHz/KAT9uzt39uz/eSHhEOvuBc7hub3KwNjnMyrFXMR/NdpJT0XXvgDH75TZFuYWsa36fOc+3qMTOmyPwKKX54kpvPSM3/iOFAbFub2SUpptIo16Dp5u4GWvIJqG409835bikPDmKgq7Q7kej0Sgrgd7e3puk7/1rPlogPyeTrKxs2cqVLN3s7CwysguYOfg+9+y9mf3kCy7jEQUJ4aRU9vGYsRvSB1kaOCUNoAra3FfUZsSSF8B5uauEqPRaArsVjYdzZGWXsKu3sD5UR0HtiHcepUdHeVIkuZ2LQcneHnjszPTWoYj6TF5H8NSh23scRr50NdLSPxM0n1e6frw6TnVtE1vnwW7+m2dPZlt5H53KrhQV4Sdux9PNRCqKODWoqS+tYEcX3Nhy6E/orK+lf277dn6pVxwbq2Pd1LT0cR5S0fmFlrw5kgvdZPK7T/1Xwa49oK4ki8iIeCYOQ9zjuaKvro4Fn9IEO6tfuqhtGQhydUtpGg5XaKipZWb7R70egRKKfUHg+Qi8aqVpPlkg7n04Axt3R8giW3lSv40UPixwW+woJqGg9bYlvjlcR7SykKPAL3fAA7u7u/JoUOnvb7N5LPSVpRCuKOL4KsAkfwAAKWTehw8fmJubC7pbtgIjP9O1dDOq0kZHURLpjRPY7deoL6RGhY2ukjTqv3gHBu1NtJBR1I7XPrHTXaBAVTkUpCxc9mssFhMrwy00DC6w3FdBZFoNV24XmpNTrqVweYGbx4FWc3HnvAejVsPV9f1l3egr43NSKZchvLOByf/o/tXxLElxiVRUVNE55R/0E5iux2FBc6kjqGghyxX4lH9fio4l/e5u12YzZv0JrdWVzG7tUpUaQ+XIJh6niVO1zuemtbO/Os/c+j7Bg57dGLQXmGzBDK2GS7Q/Gv3prqDiWBB4RgI/oDRdXBxuMjbUT+/ACLvqgMgv3yiQx2VDfXzA9u4+ektwK/obj4W8dDTTzD+flGyFcJlJ0YCkwARtbW1Bz1pOl8hUpTEws8LccBuZOSUsn9xvBTY0NJCUlIQ0//B32bZG6/kQFsv4A0fM3uUifZClgVCB2+V6n9yfuXR+0//moL8ylc+KLOobmpnZucSp3yQ5MpaW0Vk21lZoLcskr3Hitk9ta7iKiORSAtcyP1vuJfpzJDk1PXJfn/lwitjwzxRU1NDSP3078CdQlsfv2+gqTJJH5t5RwY9P6r+euDohO+YfEos7b4MY/Ncjj7ku9XNLrllpUF/wdk1PRQYRsUo6Z7x9ujOtBYTHplBd28Dk5k0ABTeLXWUoQsxpDk5PHAkCb5PAdypNO/N9TVQ1dbOysclUbx0xsaksnoV2bd2gcxiOaakopq61h/6OBrLyKtj/7sjgHsZr0omQghrc8yWTXIVSgIO7wdevzg+Ym55mbnEdnfl+ZSjNOfz8+TOLSwHrg94U5o3+NexNExf2gaqh0JPjH1JsybqXpp0Ezm291p+ztXt4G1pNSseiO2Vhbp49tbfRolnqJU6ZQe/gKCND3aiiI2ic8fffOS42SI6OZWTX30Azne8zu7CK/sZKdDs52lxmfnkLc7Ap9BDRQ97j0G6SHCPl+/OjAtnOV8nJymfzJ4Sck/r3Kyoq5PCGUl950OZ2sL+xyMr28W2fr9RHvzw/y+bhxW3DRXpmpqOIovbQ3RlBaYoDQeANEvg+pem2cbi7h9l5o60sNGfHkdX8jTBZdi112QoK2qa8Cx27zdRmJlA5vPFdWN2mQzKiwykb/PbHXeq/UalUbG9vI8VEfcgm3be+vi5bqk8VOeUh+T73PW7zKaXKCJJKOoP6Gb9HLim4hBTnVHLTfvWBvifBjYFasqr65PfDfbFOqjKTzaA+PQ9SODZFbiP6O9/8e5J8gtN2RuqySa8e9PeDP0GqgUncDFZzW9U0luQztHqnPzjw5u/cl/ovJa9JSkpK6Dm0D03XIwUvSKZ9+cbyfOiD4j5B4G0Q+D6lGaLs043ZJBV23fth2RisJjw+jxPLjaK10pyfQHrDuH90YIh0755ymqVIMv1U5KaSUtTExfV/K0JpXlpBQQE7Ow+bRC+tBCG5GFdX/Ytp35XjWY7vrOIifWxvaMryBK748mgBbQxXZ/AhLoc9/f3W92OS3dvbk7nf7d+8L43t4XqyqgfkPsuJ5mKK26eD+i/l55xmhhqKKajt5szwc0djOiw6xjuryCpt5vwmPM59wv/A+b3pHura2qkqzKP1i3f1kh9I7qtHpVHUUrCP5ubmewf/fPXQPSc8VzukJ6azItZcvYeQOP3WCXy30rRcHDLU3UZrewcj4zN0lKWhDDEpXAZ4raE48RMF3QEjH10m6jKiyW2fCf7w/wdxh1nH6sIcy+u7mOS1+/7jAd9lyXq8Gx3ovicly+ih996XxtOed3O6MUN1cQF1PTPYPQ52F8eoLMqntL4bzbWbq9NN2urKyckvY27v60Dp/yXP7mQzYWFRjIQYVPVfz37rusT9oVNfHLp9GqoraW1rprFjBN197lW3ne25ERrb+7m0/nej6Vvy3XvNbWa8p43u0QWufvLyYGtDTWQXlDG1fvSoBuS9soe48NA6CPFo0CnDxigJ6cX8BO9xUD7iQBB4qQS+S2kaDudIVyTSOLyI3mRib7aXqPd/klrlj+0ZWGD97hif3n+iZ34Po0GPTq/n8niNjIhwascePgcwMM3fad98tkHv4ATzg/VExmUxMDbI8NQSG/MDRIV9pKy5i56+ITa316lQRZBUGhxw/L9YmY8XSQr/QGnPwk/7aP+XDDfXXTYreoMpeGTozcU7f2VLO8jUvnPDDx16cLt/WuI/JNlzPrzUU05G5YC3i+U5BRF5CwLPRODxStNxSWVKFBkNY37X2dU+quj3FHR/HYZOKtfmQAX/hkWQW1xKaUkJpaVl5Gcq+ftDJCPbP39wxTOxfbJs3W6XbI1rFrr4EBZB6+SmfOy2HpL5+T1xeQ2+NSA9DJWrSA4RQ/c+YTzX53LA+7iC5qAA9vfdH3zew8ZEDwOzv9F0nGAAv82R+9rI+vwYhXmFD44O9dvAEQX9rQg8WmlerA7w6UMUkwHr9dnPVkj4FE53yIWFnQyVpRCb2yrPt/P4BuNsDFTyMTqDw6+i8HjkgSOikX/3PfQw3ZzHZ2UZF74uR8vhNFGfohne8rljXVrKk6OpGLo/ak5wqg4mGnP5EJ3BpvYmTk/wHd8+MlGTEkl2R4Db/dsPiKuvlIDbqmNtZZnjoLCDr7QwQmxB4AcIPFppLnYX8z42H01AmJjD2VY+Rqaxf7PAb5BAZhqz/fE05UtuM41Zsaiqh/3Wqu8Z494MhZWtBA2aDErvNz2Q+oAzY+VltW6chntfGvicWIjGF2fVrl4iISKW8RCrUYSidjTXRXhYJH3fOVpTmgKi+PyeytHQk/BD5SnOCQKCgCDwmgk8Uml6mGzO5mN6NX79aKUjX0Fq7cjt/K5gINKAn3gKe/2uW+PuONGf45kO8XE3HG8xvbTN7WyW4MR+2yOncY/UyAg6F24i6jgZrk4jscQ/YvlwqpXPCXmob0co34/rWr1GasQHMmoHMVitmExX8sokUnzh//yZTFzpzuitzuavd//QMn9ntYz7sxVXBAFBQBB41QQeqTThYLKF8PgcTn0f5uOFHuISM1m/uIn0cpeHi9HKdFLldQ7BZT6jOitJXufwbkSyy905aqoa2XmiKQ93JXnNx7r1AcI/JbCk8flmHVIs1giKe/yu0emWXD5n1HB+oWZmbvn+mLF2HY25Cbz7828+hn/i06dwwsMf+fv4gX/+/os///5E3xOPuH3N9SRkFwQEgbdN4NFKE5uOntoSyhvaGehupbi0ioWDb09xMJ+uUpqbT2t3L/UVpTQNzhNqpoBxa5RYZUHAXM63Df8xpdscrCUhvQqtb1K/Q7dJWkIig+t+9gczncRGx1FQXs/C3tfxeG/y2xlt4MPff/MhLExeVzEs7IMcL1aKGfuYX1jYe/79EMuXA3+Enps8xF9BQBAQBN4igccrTWmBd7dTDnCt1mi5dty1F0NjctrMXJyfozdZ752XOddZRn5zwFJIoZP6Lc86HXZsdn8YHI/bxfW1DVfgNEWPkyu9HtP1t4MTXGmO2dzakqMkSZGStgL2peOH/qRgEVvb+1zZHvYO/JYVJwotCAgCb4rAdynNn0LAcUlFmpLBTe1PSV4kGprAwWwftT0hIu+Evj3orBSP6GZQUtAFcSAICAKCwBsl8GKUpnFnjMTUEm667N4o7xdXrMOlCUYWdh+l/FzXRpbHOskpawgaRf3iCicEEgQEAUHgiQm8EKXpYbq1iILWyWePSPPEfF9uch4n6sMdVte30ZsdD5bz+vKA/p5OGssyCVMUcP5zw78+WC5xoyAgCAgCv4LAi1CatsttCrPzWFZbfkWZRR4+AifL/URFJLKk9gY2sGp2Gejtoa+vL+jX093L/NapbI267FasTg+69RFiVfmIKhOvkyAgCPxOBJ5daTr0R3S0tDC3c/47cX8RZb1Y6ydBWYLGN47Hpj9jdmZaXqBYWqT45jc9PcPW8WWQC1e7PkSMUJovoh6FEIKAIPDrCDy70nQ77VgDRoX+uqKLnBbbC0muHLh1iUsjcu02G3a7/aufM2iYLgilKd4fQUAQ+B0JPLvS/B2hv4wyW2jOTaY6YJUZ494s+TlZ5OTmyuuJSmuKStj5ErMAABToSURBVL+szBw6xjdulaskv259mNgU0af5MupSSCEICAK/ioBQmr+K9EvLx3ZOniKWhuEZFpfWMEsuWqeNK6MR492fwYjF5p/76XY72Bip431kCpvnFqQlusQmCAgCgsDvQEAozd+hlkOW0c78YDNlNc2sHvmjCoW8NeCk51rH5FAvTfU1lJdX0dTawcTizj1xhwMeFLuCgCAgCLwBAkJpvoFKFEUQBAQBQUAQ+DUEhNL8NZxFLoKAICAICAJvgIBQmm+gEkURflMCHhfXcl+zB7Nei8nmwG5zBE0Nei1kDDvz9E6uYb86oq1nzNvH/lqEfyFyqjem6J/aBbuGjq5+sSbxT6oXoTR/EliRrCDwUwk4TcwMdfNl5RBwMNnVweaFlpXxATksYugQ+jZG68vpmjt4lGgep5XjgyNMdu/qAGujrTT0LXy1gPyjEpXGnZmPqSsqY/3CzEZ/HRllrRzvL1JYWsfG7j5be4dY7llY13q5Q21hFkV1g/xISBSPzcTh4TFWZ+DKB48tyU++32FivK2K9JwStrT+AXlSrk7tFiVF1RwYr5jpqiS7ogf9xQ4lxRUsb++ztbOPyR76bbhfajeLg820jCwHjZi///5ff8VlM3Ek1ZtUNNcVHdWljGyc/RJBhNL8JZhFJoLAUxLwsDrcQuuXDTlR3fEmUwub2KTvvsfKSGsNoyHXOHVzeXrEueFxasZp2EAZmciML2biRGM2qvLBHx785bQcU19SwcalmY2BBrIq2jk5XCFTlUbPxCLTg600DcyHVM7zLblEZVSzr9aHvP5Q2o6TeaJjU9jWBSujhz7/K+4z7o4SGaFgdH0fqyN4pLrHbubw4IRrh5Xpripyq/owaPfJT02hdWSO+bEuajvGeVzpXAxVZZJZP/ZDbH8mG8vxPPGxSjZ10kvvQn18yKXpvjWdn1YSoTSflqdITRD46QQ81lPK88s4kOcJwclMJ9lVPXiDIYJhe5Kimm6+DgvsxnipQRfwcbFcXbK/u8v+wQlmh3/pOX8hnByvjpAYEUfn7AZWu4OZlnwyqoexOi0c7u1yeKLBZ4T6HvNgNlywt7PD4ekl9xpxHjcOu112J18uD1PVPc21fov8siaMLvCoVyisaMZ4xwi06s9oLUpGUdKO+vLq9sNuudKyv7vDwYmGu0WxWw2yrLt7B+jMvo+ry87ebA9RMUmMLO5gtru40qo51/sbFW67hdPTM6TV7xwWPadqHQ67meMjv+XtcV6jPj5ASlt7k7aPhMN2xdH+Hrt7h+jMNzXkp+vf83Ar/7Eamw+a22ZieaieyJgM5vZPub5jeUt5a84vkKJHH8z1Ute7DNfHFJVUI0fHNOxRUlrN+T1a02rWcyDV/9EZ1ttlHl2M1OaQ1zyB1WZif2+XI/Ulgfra47RzfnTA7u4eaq3pjkUqvWdn7O7scnLhb9S4nVbUp2qsDjvnp8ecn59zdHKCyeGvYI/Twsnxya117LQYONzflfnpb9i67ezMdBMbk8DQ4j5Wmw3DpQaj1V9I57WF00Mvd33AeYfFyIlaK793GrnODjFa/M/56+P+PaE072cjrggCv5aAx8Hu0iTDk4vePj33NWuz48zvBLudrCezZJa2YvF53ZaHGmgYXPfLqt0ks7A8RDB9Ky2ZiZQNei3Uk6VhMlNUpKSmkpKsJCW3gu3LYFXrtp3TkK3kw/swopXpTO9dstBZTEJqPmWl+aSmphAfE0tRyxef0nazPdWFKklBsiqFJIWCgtoeDLIZ7BdR2nOaDyjPymNF60S3MkRl5xTXui2KKlu58oBLvUJRVetXSnN1oI7oz2GEfY6mqHZQzvdoYYA0ZZI3zyQFORVtXPgA6ffnyU1RkKRMIUWZRGJKNlO7OjAdUZKRyPsPH4lX5rCsNjBSmUJm6+ytoObjOdmi2b0CzUIbEdEpVFQWosoqYktaEd50SmNhJgqJn0qJIiWb0TVvfZnP1inLTiM5JZVUlQplShZjm6HChXrYn+sjTalAlZKKSqkko6iOI7ML8+EcqYlRfAgLR5FZ/JV71qFeIC4ymQ2Tg+P5Hmp6JKV5RElZrVdRGvYoLasNuXrUycoImaokVKpUUpTJpOVVsiXXv5ux+nyZU0FeNqkpKrmOi5tHsUiGrlVDa0UuqmSVXP8S1+aRVa/i9NiZ665FqVCgSlWhSEry1itgu1whJTKa4spK0tNz6BvqRxn5iZZZqYvBu12s9BMZl8mhxYPxYIFclQKFSoUqOYnE5Gym9/V4ro4pS4/j/YcwYlUFbOwfUJGWQPOst9vBfLZBaXYqyZJ8KckkpeYytqGWM7hY7iMiXklNeRmZGWkkxceQmFnB0dXDF60QSvOmtsRfQeCZCVzuLzM81EuGQsHQxgkbE50UFZUwsLAfJJl5f4LU8jassqfOTm91Pn0rJzjdPtedbouMwvIQwfSttOUkUTGyDZgoU8VQPexVoJJbd7ythrbJraC8pAOnYZPUGCVzPvfsbHs+f4YrmNq9kO+92pskOkrB6qUD6/EsidEKvuxo5Gtu0xmVaXEU9y59la7japv06Hhmzh3olgco75jkWrdJfnkzRg+4z5YoqGjG4DdEbtOYasgitX5cPraql0mOiWdg5cR7/fqSxhwFOa3Tcn9va04cKVUDeO1LO2P1OSiLOuRj5+kC8Qlp7Oikj6aHocoUstvnbvMxH8+jSFAhKc3LpU7+/ucTbVPbPsvKRl9FOinl3dx8c/en2omOz+LIbGeiIQtlSa9sBYKbnekeKpqHbz0CN5lYzpZJioyia97X1+ww0l2airKsR3aBGzYHiUko4OyOlSk971AvkhCTyobJyeFsF1VdS7LSLCyp5lwqsGGXYmn/jjFl126RFhNJ0/iWd+CYy8xwbTbx+a1Itv94Yw7/RqYyf6CTxTQezqH49InOlTNO5zuIVhSg9hnsxr0FissbuLCDerGL6PhM1tVm+TmrZovM+Bg6Fs9wGDdRvv+XnKZR30AvJwPlKSSWdPvcx25GajLJaZqUXa7tOTEoK4d83QBOxusySCzoko+tJ4soElLY0gNuA+WpibTOHcvvdX1mPFn1w1jk98bF1lgTkXHZHFhBvz7Ep/fvqRtZkevFY1VTpIiidjL4f+ymbkL9FUozFBVxThB4BgJut2Q6upmsz0eZVUjn6KLvQx8sjNOwTX5etfyRkgYBfWkupaSx/9aysp8skVfagP6r8R8+pTksKUY7XSWpRCuz6fsyy8HZBdf3+FGdeklpJjGr9lqh0y15KEu7/bKZD0mPUzB5bGClq5CY9GoONRecnZ6iudAw0ZhLZGbjV+5ix9UOGTGJzJzZcV5uU1/fzMraIq1dw5glpXm5TVPnEKYQSnOyIYuU+i8ymM3+MiKVJeyea1DLeV4w11nCJ1UlRqcL9cEOJxcGrq0WdJdqhupzURS0yvJIfZpxCWm+Pk0PQxWhlGaKrDQvFtr5nJDHiU8BOQ27pEZF0ja1xYVGzdnZOWf7i/K5/s0LtkZq+RyZSFPvKJsHJ5hsodzfsDVUSVRqFfqA7krtxgCR0WnsW8G05VWap4H+Ud8rcaM0168cWM9Wqa1pY/9oh7aOXrRSO8B0TGtbt3c/4DU6mmwkQlHEeYCBZTqYICZSyZbhmvH6XLKC+jSd9JelkN00yfneJPEREZQ0drO8uYfedON2dtBbqCS5oofziwvZrX1xcUp7fhIp9eNc6zdJiohl8tCrUCVxzhZ7iIhKZUdyLVyfkJekZGzPICvNs/0dTi6N2KxmdBfnDNdlk5jbJjc6rCcLcmNmS4LmU5pti6fYLpZIiEhg4czvMXFfn5ATF0XX2iXGjUEiE3M59WpUOZ/eEiUFkoX+wE0ozQeCErcJAr+KwNFEA5+TioM+aFLeHpcdy7UdPDYG68sY3fFGcnI57NgCFj1YG26mcXg1hLg3StNrXbpsBhbGeikryEYRH48ys4hFn2UR+PBdpTnVnEd61bB/IJDpkPR4BZMnemZbcnkfHolKpSI5OVn+paSkUFDRhSFAKUjp3yjN6RPvB87jceN+YEjGQKW51FHE+48RJN/JM7e4Fa3TxdHiCAW5WWRLcZQzMklLjie5pEP++P630pQ+zn6lGZGQz6lP0Vxr11B9/EBMohKVr6ySS1qVnsXwmuSGldztE9RXFJGcmEBCcjq9MztfTQmabcpGUdjt5wno98aIjkpm++phSnNN7zX7PO6HMVztLiY2qznI6jWfzREXmcjqpYXx+nzKuuYDXgMPXxpzyKwdla1szf4KHQ1VpCUnEpegpKprEgfXdOQq+BgZG1D/KqT6L+2axaJdRxkZy9SRX2li01CkiKZl/gTd2gBJ2TUY5baFm/3FEYpyMuVY2Bnp2WQkx6Es9HoIQinN9sVTjLsjRMVksCf7kX3i2zQUJEbRuqjGuD5AVGoxOp+VDE56ZKW5ElDWb+8KpfltPuKqIPBrCTiMDNUVEKPM48jfWJZl0G9P0zqyKH+0rJodWpta2TiT/FM3m4vj1UkaWvvR2u5oKPmWG/fsFth1DPYP+Ps9HVf0lqagqhn0uR5v0gRJaaZI7lmNVyCv0rxxm0nWzI3SNLLRX0p0Rj2m28c97Ez30TG+dnvmZueu0rw5/5C/gUpzZ6SayORytAEW6eHiMG2jK5jVKygiomidWONS75VqqbuUpAKvxeJVmuk+9yyyezarxd+nKS2fFxGtYs8EkqUpK02fpem6OiA9JoreVa8rWpLbZTqls6mNfe0Vi6O9rPqsczxO1gZriFIVcXnHVSrJLzWSNAFW395YPZHKUnQeMPrcs9+yNG+U5kPYSfeczLTwKTYn6B07neuQy6dxOBmvz0ZZ2uMfdeux0JSTSFn/Cgdr43xZ9rnC8XCxMUZkVBKbWjOjlakkVw8HNAzszA90MLp2ik2/TtJdpYmH6eZ8Usra6KkrpNbXXWA7WyIxIpr2iVUuDd56W+kqJjGvTfZwWI+9jZktySgNsDTt2hXiP8cweXB1i8J+vowiMp6pUyuGtX5ZaWpvjGOhNG85iR1B4FUR8NilkYWHjPd2MD47S36akuH1E86OvXPRPA4zWytLHFz41dG1UcPBseZ29Cjuaw539rmShnqG3Ky0ZiVSNrQJXNNSqKKgcZhznQHt6TZlaQrK+r7ue5TcwaqIaLqXDpCswanmXFIrA6acmA5IjY3ny5ERp26LtLgY6gcW0BoN7CwMooiKpW8leDCTJN6PKM2JugySa8fkUjr1e+QmxlDZNcWlwcDB6hdUMTG0zx1jv1wl4VMEbTPbGA1aNmaGyVZEEJlZjeYanOpFYqMS+LJxKjcWNoYqZRfs+pkWzcEqFZmJ/Bubyr40EGi+lU9xuZzeKj0XU61FxKWVsn58gV5zREdpOgm5TZg8sNRTRlJOLbvqSwzaM/pqckguaiPAzpLlt2u3SYv9TFHbGOdaA2fbs2QkRFMz6vUI6DcGiIrL4+Qe92xclIpV/a0WCFnzd0+6jIfkJkaQUz/A2aWe8/1l8pQxlPYuyreONeTwz8c4OidX0Om0LA7WEx2XzqbBweX6IAlJmcztnGIwaFkaaCBamcupFYy748RHJ9Az5+W9PNxMdEyy7C516FZJ/BTDZKClKY0rkgZbRXwkMj6LTb0Xru10ifjPEXRMb2I0almbGSEnIYKY1Cp5/V/b6SIJUbF82dSATUtpSgItc0fye91RpCQuq4qdE6lODmkpSiGpuF12x18u9xKhKiRQaXYVKcjvFu7Zu++IOBYEXjQB6/kaOclK2iYlheZhpqOC1OxiRhZ3vUrRqqEyP5OpgP6gxxfomv7yXJomd+VHbZd7NJZmk5CQgEKhoqb7C/pQFqrHymhjEdFxSXzZvmB1oIbilgm/sracUJyZzcyx1OwH3f4y1YWZJCTEo8oslMsQqjfvR5TmfGcphR1+i/DqeIN6uSzxKNPz6J/Z8FlJTjYmuklXJpCgSKGmc5jtzXlyVYk0jkkDosz0VOUSE5vKzIlJGuJJd00hifFxpORVMDU1QXFxCYcm0K71o8qqCnKbexwmpnqbSFMmkqhQUtLQw6nB5/u71jHcXI5CkUhigoKC6jYOdKGVm/5ghZoiiVkCSlUW7WOL3on7eBVRelY15yEGAjkv1slOL2TLeOtvfPBrcXWySUNpjpxnUnIGzYMzvgFNLqY6qqlp7aO9rojEhATScspZOPAO/JL6w5fH2klPUsjPphZUsrh/s+iDm6PVSYqzUuRr6QXlzG57RwzbdFvkpmSwcOqf0iML67qiNV9FbvOYb9CUdNbB+mQ36YoE4pUp1HSMsLs1T64ynvqJXXCb6a7KIjIhk5W9Q1pL8+j1Wb9O0zl9TeUkJyTIdVLa1MuZ0auMtRujpBbW4fNmS8PcGK7Po2ZE+r972Cbcsw/jJO4SBH4qAY/LidV6fesa9bidWCxWbgbEus0nVOSVcWgN8EF+h0TSYKPgfkM3TqeTu4uMf5W0x4PL5X1W7je7EUy+0YPb5SKwO1KySKV0XUH3BafqNO2SGZvIlK9PM/jqt4++lkFqa9yfpySfU5LRl6y04PqNbJKsLqfzlrVUEOnYe92D2+32Pifd5/Lt3xHP7ZIY+tP3X5a4OXE6Q13z3yXt3TK7s+C7LM89+Xqv/XfawTkFHoWuf4mvR6o7j0euR3eIepSZytxuqPrTlZ6X6j/4XfO+Q1/fjcz43jwCuMrvr08Wb71538mv32vkOrxbJ9IyhlIdBm6BaQaev29fKM37yIjzgsALInC1P0FmWQehDMEXJOYjRPGgPZiRgybMSH49sQkCr4SAUJqvpKKEmL83gZ2hJsp6/O7I107j+mKdrMRYcuv6faMlX3uJhPy/CwGhNH+XmhblfMUEbHRXFtC//PVgmtdaKI/Hhe3adusifa3lEHL/fgSE0vz96lyU+BURkAaazA6209A5giHUaJpXVBYhqiDwFggIpfkWalGU4e0ScLswm0w4XKGGT7zdYouSCQIvlYBQmi+1ZoRcgoAgIAgIAi+OgFCaL65KhECCgCAgCAgCL5WAUJovtWaEXIKAICAICAIvjoBQmi+uSoRAgoAgIAgIAi+VgFCaL7VmhFyCgCAgCAgCL46AUJovrkqEQIKAICAICAIvlYBQmi+1ZoRcgoAgIAgIAi+OwP8Ptb+9TVacookAAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression using batch gradient descent\n",
    "\n",
    "To implement Gradient Descent, you need to compute the gradient of the cost function\n",
    "with regards to each model parameter $\\theta_j$.\n",
    "\n",
    "Once you have the gradient vector, which points uphill, just go in the opposite direction\n",
    "to go downhill. This means subtracting $\\nabla_\\theta \\mbox{MSE}(\\theta)$ from $\\theta$. This is where the learning rate $\\alpha$ comes into play: multiply the gradient vector by η to determine the size of the downhill step\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta^{(next step)} = \\theta - \\alpha\\nabla_\\theta\\mbox{MSE}(\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "Recall the expression for gradient calculation:\n",
    "\n",
    "\n",
    "![image.png](attachment:42f4187a-35c2-4c38-97de-821f48cf037a.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha is our learning rate\n",
    "alpha = 0.1\n",
    "n_iterations = 1000\n",
    "# ms is the number of instances\n",
    "m = X.shape[0]\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - alpha * gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the value of $\\theta$ found by gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s exactly what the Normal Equation found! Gradient Descent worked perfectly.\n",
    "But what if you had used a different learning rate $\\alpha$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_path_bgd = []\n",
    "\n",
    "def plot_gradient_descent(theta, alpha, theta_path=None):\n",
    "    m = len(X_b)\n",
    "    plt.plot(X, y, \"k.\")\n",
    "    n_iterations = 1000\n",
    "    for iteration in range(n_iterations):\n",
    "        if iteration < 20:\n",
    "            y_predict = X_new_b.dot(theta)\n",
    "            style = \"b-\" if iteration > 0 else \"r--\"\n",
    "            plt.plot(X_new, y_predict, style)\n",
    "        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "        theta = theta - alpha * gradients\n",
    "        if theta_path is not None:\n",
    "            theta_path.append(theta)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    plt.axis([0, 2, 0, 15])\n",
    "    plt.title(r\"$\\alpha = {}$\".format(alpha), fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "#subplot arguments: Either a 3-digit integer or three separate integers describing the position of the subplot: nrows, ncols, and index i\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(131); plot_gradient_descent(theta, alpha=0.02)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.subplot(132); plot_gradient_descent(theta, alpha=0.1, theta_path=theta_path_bgd)\n",
    "plt.subplot(133); plot_gradient_descent(theta, alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is plotted in black.\n",
    "Line with $\\theta$ from the first iteration is in red.\n",
    "Lines with $\\theta$ from the next 20 iterations are drawn in blue.\n",
    "\n",
    "On the left, the learning rate is too low: the algorithm will eventually reach the solution,\n",
    "but it will take a long time. In the middle, the learning rate looks pretty good: in\n",
    "just a few iterations, it has already converged to the solution. On the right, the learning\n",
    "rate is too high: the algorithm diverges, jumping all over the place and actually\n",
    "getting further and further away from the solution at every step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial regression\n",
    "\n",
    "What if your data is actually more complex than a simple straight line? Surprisingly,\n",
    "you can actually use a linear model to fit nonlinear data. A simple way to do this is to\n",
    "add powers of each feature as new features, then train a linear model on this extended\n",
    "set of features. This technique is called Polynomial Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at an example. First, let’s generate some nonlinear data, based on a simple\n",
    "quadratic equation:\n",
    "\n",
    "$y=2+x+0.5x^2+\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot and see our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, a straight line will never fit this data properly. So let’s use Scikit-Learn’s PolynomialFeatures class to transform our training data, adding the square (2nd-degree polynomial) of each feature in the training set as new features (in this case there is just one feature):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the dimesion of our original data, and the one after adding the quadratic term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape\n",
    "X_poly.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_poly now contains the original feature of X plus the square of this feature.Let's confirm this by looking at the first element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0], X[0]**2, X_poly[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can fit a LinearRegression model to this extended training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the values of estimated parameters and compare to the true ones: $y=2+x+0.5x^2+\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when there are multiple features, Polynomial Regression is capable of finding\n",
    "relationships between features (which is something a plain Linear Regression\n",
    "model cannot do). This is made possible by the fact that PolynomialFeatures also\n",
    "adds all combinations of features up to the given degree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "X_new_poly = poly_features.transform(X_new)\n",
    "y_new = lin_reg.predict(X_new_poly)\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Predictions\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you perform high-degree Polynomial Regression, you will likely fit the training\n",
    "data much better than with plain Linear Regression. For example, the following figure applies\n",
    "a 300-degree polynomial model to the preceding training data, and compares the\n",
    "result with a pure linear model and a quadratic model (2nd-degree polynomial).\n",
    "Notice how the 300-degree polynomial model wiggles around to get as close as possible\n",
    "to the training instances.\n",
    "\n",
    "Note the use of Pipeline operator that makes it easier to sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "for style, width, degree in ((\"g-\", 1, 300), (\"b--\", 2, 2), (\"r-+\", 2, 1)):\n",
    "    polybig_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    std_scaler = StandardScaler()\n",
    "    lin_reg = LinearRegression()\n",
    "    polynomial_regression = Pipeline([\n",
    "            (\"poly_features\", polybig_features),\n",
    "            (\"std_scaler\", std_scaler),\n",
    "            (\"lin_reg\", lin_reg),\n",
    "        ])\n",
    "    polynomial_regression.fit(X, y)\n",
    "    y_newbig = polynomial_regression.predict(X_new)\n",
    "    plt.plot(X_new, y_newbig, style, label=str(degree), linewidth=width)\n",
    "\n",
    "plt.plot(X, y, \"b.\", linewidth=3)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized models\n",
    "\n",
    "Let's now check the effect of regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's generate data corresponding to sine curve (between 60° and 300°) with some random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)  #Setting seed for reproducibility\n",
    "\n",
    "#Define input array with angles from 60deg to 300deg converted to radians\n",
    "x = np.array([i*np.pi/180 for i in range(60,300,4)])\n",
    "y = np.sin(x) + np.random.normal(0,0.15,len(x))\n",
    "\n",
    "plt.plot(x,y,'.')\n",
    "plt.xlabel(\"$x$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", fontsize=18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "def poly_regression(model_class, x,y, models_to_plot,power, alpha, param,**model_kargs):\n",
    "    polybig_features = PolynomialFeatures(degree=power, include_bias=False)\n",
    "    std_scaler = StandardScaler()\n",
    "    poly_reg = model_class(alpha, **model_kargs) if alpha > 0 else LinearRegression()\n",
    "    polynomial_regression = Pipeline([\n",
    "            (\"poly_features\", polybig_features),\n",
    "            (\"std_scaler\", std_scaler),\n",
    "            (\"poly_reg\", poly_reg),\n",
    "        ])\n",
    "    polynomial_regression.fit(x, y)\n",
    "    y_pred = polynomial_regression.predict(x)\n",
    "   # if power in models_to_plot:\n",
    "    if param in models_to_plot:\n",
    "        plt.subplot(models_to_plot[param])\n",
    "        plt.tight_layout()\n",
    "        plt.plot(x,y_pred,'r')\n",
    "        plt.plot(x,y,'b.')\n",
    "        plt.title(f'Plot for param:{param}')\n",
    "    #Return the result in pre-defined format\n",
    "    rss = sum((y_pred-y)**2)\n",
    "    ret = [rss]\n",
    "    ret.extend([poly_reg.intercept_])\n",
    "    ret.extend(poly_reg.coef_)\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first estimate the sine function using polynomial regression with powers of x from 1 to 15, without any regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#Initialize a dataframe to store the results:\n",
    "col = ['rss','intercept'] + ['coef_x_%d'%i for i in range(1,16)]\n",
    "ind = ['model_pow_%d'%i for i in range(1,16)]\n",
    "coef_matrix_simple = pd.DataFrame(index=ind, columns=col)\n",
    "models_to_plot = {1:231,3:232,6:233,9:234,12:235,15:236}\n",
    "x=x.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(1,16):\n",
    "    coef_matrix_simple.iloc[i-1,0:i+2] =  poly_regression(LinearRegression,x, y, models_to_plot , power=i,  alpha=0, param=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the error and the value of coefficients. Column rss corresponds to  sum of square of errors between the predicted and actual values in the training data set. Clearly, as the model complexity increases, the models tends to fit even smaller deviations in the training data set. The error decreases and the size of coefficients increases with increase in model complexity,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2g}'.format\n",
    "coef_matrix_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression \n",
    "\n",
    "Ridge regression performs L2 regularization, i.e. it adds a factor of sum of squares of coefficients in the optimization objective. \n",
    "Thus, ridge regression minimizes the following:\n",
    "Cost function = RSS + $\\alpha$ * (sum of square of coefficients)\n",
    "Here, $\\alpha$  is the parameter which balances the amount of emphasis given to minimizing the original cost function vs minimizing sum of square of coefficients.\n",
    "\n",
    "Let's look at the effect of alpha for our data and polynomial degreees up to 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_ridge = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]\n",
    "\n",
    "#Initialize the dataframe for storing coefficients.\n",
    "col = ['rss','intercept'] + ['coef_x_%d'%i for i in range(1,16)]\n",
    "ind = ['alpha_%.2g'%alpha_ridge[i] for i in range(0,10)]\n",
    "coef_matrix_ridge = pd.DataFrame(index=ind, columns=col)\n",
    "\n",
    "models_to_plot = {1e-15:231, 1e-10:232, 1e-4:233, 1e-3:234, 1e-2:235, 20:236}\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(10):\n",
    "    coef_matrix_ridge.iloc[i,] = poly_regression(Ridge,x, y, models_to_plot , power=15,  alpha=alpha_ridge[i], param=alpha_ridge[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As the value of $\\alpha$ increases, the model complexity reduces. Though higher values of alpha reduce overfitting, significantly high values can cause underfitting as well. ANd what abou the value of coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_matrix_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error increases with increase in $\\alpha$, as the model complexity reduces.\n",
    "Even a small value of $\\alpha$ such as $1e-15$ gives us significant reduction in magnitude of coefficients. \n",
    "Compare the coefficients in the first row of this table to the last row of simple linear regression table.\n",
    "High $\\alpha$  values can lead to significant underfitting.\n",
    "Though the coefficients are very very small, they are not zero. Let's confirm this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_matrix_ridge.apply(lambda x: sum(x.values==0),axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "Least Absolute Shrinkage and Selection Operator Regression (simply called Lasso\n",
    "Regression) is another regularized version of Linear Regression: just like Ridge\n",
    "Regression, it adds a regularization term to the cost function, but it uses the l1 norm\n",
    "of the weight vector instead of half the square of the l2 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_lasso = [1e-15, 1e-10, 1e-8, 1e-5,1e-4, 1e-3,1e-2, 1, 5, 10]\n",
    "\n",
    "#Initialize the dataframe to store coefficients\n",
    "col = ['rss','intercept'] + ['coef_x_%d'%i for i in range(1,16)]\n",
    "ind = ['alpha_%.2g'%alpha_lasso[i] for i in range(0,10)]\n",
    "coef_matrix_lasso = pd.DataFrame(index=ind, columns=col)\n",
    "\n",
    "#Define the models to plot\n",
    "models_to_plot = {1e-10:231, 1e-5:232,1e-4:233, 1e-3:234, 1e-2:235, 1:236}\n",
    "plt.figure(figsize=(10,10))\n",
    "#Iterate over the 10 alpha values:\n",
    "for i in range(10):\n",
    "    coef_matrix_lasso.iloc[i,] = poly_regression(Lasso, x, y, models_to_plot , power=15,  alpha=alpha_lasso[i], param=alpha_lasso[i],normalize=True, max_iter=1e5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model complexity decreases with increase in the values of $\\alpha$. Again, let's check the error and the coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_matrix_lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the same values of $\\alpha$, the coefficients of lasso regression are much different than that of ridge regression (compare row 1 of the 2 tables).\n",
    "As $\\alpha$ increases the error really increases\n",
    "Many of the coefficients are zero even for very small values of alpha. Again, let's see how many are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_matrix_lasso.apply(lambda x: sum(x.values==0),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that even for a small value of $\\alpha$, a significant number of coefficients are zero. \n",
    "For values of $\\alpha=1$ and larger all the coefficients are zero! (except the intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice question\n",
    "\n",
    "For the data below (x,y), plotted in blue, fit the polunomial regression model. \n",
    "Test different polynomial degrees. Try Ridge and Lasso with different $\\alpha$ values.\n",
    "How well does it predict the values of y_test for input x_test (red points)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "m=100\n",
    "x = 10 * np.random.rand(m)\n",
    "y = np.cos(x) + 0.3 * np.random.normal(0,0.5,m)\n",
    "plt.plot(x, y,'b.')\n",
    "n_val=20\n",
    "x_test = 10 * np.random.rand(n_val)\n",
    "y_test = np.cos(x_test) + 0.3 * np.random.normal(0,0.5,n_val)\n",
    "plt.plot(x_test, y_test,'r.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "Logistic Regression (also called Logit Regression) is commonly\n",
    "used to estimate the probability that an instance belongs to a particular class\n",
    "(e.g., what is the probability that this email is spam?). If the estimated probability is\n",
    "greater than 0.5, then the model predicts that the instance belongs to that class\n",
    "(called the positive class, labeled “1”), or else it predicts that it does not (i.e., it\n",
    "belongs to the negative class, labeled “0”). This makes it a binary classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Probabilities\n",
    "\n",
    "Just like a Linear Regression model, a Logistic Regression\n",
    "model computes a weighted sum of the input features (plus a bias term), but instead\n",
    "of outputting the result directly like the Linear Regression model does, it outputs the\n",
    "logistic of this result\n",
    "\n",
    "Logistic Regression model estimated probability (vectorized form):\n",
    "\\begin{equation}\n",
    "\\hat{p} = h_\\theta(x)=\\sigma(\\theta^T\\cdot x)\n",
    "\\end{equation}\n",
    "\n",
    "The logistic—also called the logit, noted σ(·)—is a sigmoid function (i.e., S-shaped)\n",
    "that outputs a number between 0 and 1:\n",
    "\\begin{equation}\n",
    "\\sigma(t) = \\frac{1}{1+\\mbox{exp}(-t)}\n",
    "\\end{equation}\n",
    "First, let's plot the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(-10, 10, 100)\n",
    "sig = 1 / (1 + np.exp(-t))\n",
    "plt.figure(figsize=(9, 3))\n",
    "plt.plot([-10, 10], [0, 0], \"k-\")\n",
    "plt.plot([-10, 10], [0.5, 0.5], \"k:\")\n",
    "plt.plot([-10, 10], [1, 1], \"k:\")\n",
    "plt.plot([0, 0], [-1.1, 1.1], \"k-\")\n",
    "plt.plot(t, sig, \"b-\", linewidth=2, label=r\"$\\sigma(t) = \\frac{1}{1 + e^{-t}}$\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.legend(loc=\"upper left\", fontsize=20)\n",
    "plt.axis([-10, 10, -0.1, 1.1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boundaries\n",
    "Let’s use the iris dataset to illustrate Logistic Regression. This is a famous dataset that\n",
    "contains the sepal and petal length and width of 150 iris flowers of three different\n",
    "species: Iris-Setosa, Iris-Versicolor, and Iris-Virginica:\n",
    "\n",
    "Let’s try to build a classifier to detect the Iris-Virginica type based only on the petal\n",
    "width feature. First let’s load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "list(iris.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, 3:]  # petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.int)  # 1 if Iris-Virginica, else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s train a Logistic Regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "log_reg.fit(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"coeffecients:    \", log_reg.coef_)\n",
    "print(\"intercept:\", log_reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at the model’s estimated probabilities for flowers with petal widths varying\n",
    "from 0 to 3 cm.\n",
    "\n",
    "Note that this time we are  using predict_proba method and not predict.\n",
    "In the case of a binary classifier:\n",
    "- predict will give the predicted class\n",
    "- predict_proba gives you the probabilities for the classes. The number of probabilities for each row is equal to the number of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "\n",
    "plt.plot(X_new, y_proba[:, 1], \"g.\", linewidth=2, label=\"Iris-Virginica\")\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2, label=\"Not Iris-Virginica\")\n",
    "plt.xlabel(\"Petal width (cm)\", fontsize=14)\n",
    "plt.ylabel(\"Probability\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The petal width of Iris-Virginica flowers (represented by triangles in the plot below) ranges from 1.4\n",
    "cm to 2.5 cm, while the other iris flowers (represented by squares) generally have a\n",
    "smaller petal width, ranging from 0.1 cm to 1.8 cm. Notice that there is a bit of overlap.\n",
    "Above about 2 cm the classifier is highly confident that the flower is an Iris-\n",
    "Virginica (it outputs a high probability to that class), while below 1 cm it is highly\n",
    "confident that it is not an Iris-Virginica (high probability for the “Not Iris-Virginica”\n",
    "class). In between these extremes, the classifier is unsure. However, if you ask it to\n",
    "predict the class (using the predict() method rather than the predict_proba()\n",
    "method), it will return whichever class is the most likely. Therefore, there is a decision\n",
    "boundary at around 1.6 cm where both probabilities are equal to 50%: if the petal\n",
    "width is higher than 1.6 cm, the classifier will predict that the flower is an Iris-\n",
    "Virginica, or else it will predict that it is not (even if it is not very confident):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "decision_boundary = X_new[y_proba[:, 1] >= 0.5][0]\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(X[y==0], y[y==0], \"bs\")\n",
    "plt.plot(X[y==1], y[y==1], \"g^\")\n",
    "plt.plot([decision_boundary, decision_boundary], [-1, 2], \"k:\", linewidth=2)\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris-Virginica\")\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2, label=\"Not Iris-Virginica\")\n",
    "plt.text(decision_boundary+0.02, 0.15, \"Decision  boundary\", fontsize=14, color=\"k\", ha=\"center\")\n",
    "plt.arrow(decision_boundary, 0.08, -0.3, 0, head_width=0.05, head_length=0.1, fc='b', ec='b')\n",
    "plt.arrow(decision_boundary, 0.92, 0.3, 0, head_width=0.05, head_length=0.1, fc='g', ec='g')\n",
    "plt.xlabel(\"Petal width (cm)\", fontsize=14)\n",
    "plt.ylabel(\"Probability\", fontsize=14)\n",
    "plt.legend(loc=\"center left\", fontsize=14)\n",
    "plt.axis([0, 3, -0.02, 1.02])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.predict([[1.7], [1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure shows the same dataset but this time displaying two features: petal width\n",
    "and length. Once trained, the Logistic Regression classifier can estimate the probability\n",
    "that a new flower is an Iris-Virginica based on these two features. The dashed line\n",
    "represents the points where the model estimates a 0.5 probability: this is the model’s\n",
    "decision boundary. Note that it is a linear boundary. Each parallel line represents the\n",
    "points where the model outputs a specific probability, from 0.15 (bottom left) to 0.9\n",
    "(top right). All the flowers beyond the top-right line have an over 0.9 chance of\n",
    "being Iris-Virginica according to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.int)\n",
    "\n",
    "log_reg = LogisticRegression(solver=\"liblinear\", C=10**10, random_state=42)\n",
    "log_reg.fit(X, y)\n",
    "\n",
    "x0, x1 = np.meshgrid(\n",
    "        np.linspace(2.9, 7, 500).reshape(-1, 1),\n",
    "        np.linspace(0.8, 2.7, 200).reshape(-1, 1),\n",
    "    )\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X[y==0, 0], X[y==0, 1], \"bs\")\n",
    "plt.plot(X[y==1, 0], X[y==1, 1], \"g^\")\n",
    "\n",
    "zz = y_proba[:, 1].reshape(x0.shape)\n",
    "contour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)\n",
    "\n",
    "\n",
    "left_right = np.array([2.9, 7])\n",
    "boundary = -(log_reg.coef_[0][0] * left_right + log_reg.intercept_[0]) / log_reg.coef_[0][1]\n",
    "\n",
    "plt.clabel(contour, inline=1, fontsize=12)\n",
    "plt.plot(left_right, boundary, \"k--\", linewidth=3)\n",
    "plt.text(3.5, 1.5, \"Not Iris-Virginica\", fontsize=14, color=\"b\", ha=\"center\")\n",
    "plt.text(6.5, 2.3, \"Iris-Virginica\", fontsize=14, color=\"g\", ha=\"center\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.axis([2.9, 7, 0.8, 2.7])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression\n",
    "\n",
    "The Logistic Regression model can be generalized to support multiple classes directly,\n",
    "without having to train and combine multiple binary classifiers. This is called Softmax Regression, or Multinomial Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s use Softmax Regression to classify the iris flowers into all three classes. Scikit-\n",
    "Learn’s LogisticRegression uses one-versus-all by default when you train it on more\n",
    "than two classes, but you can set the multi_class hyperparameter to \"multinomial\"\n",
    "to switch it to Softmax Regression instead. You must also specify a solver that supports\n",
    "Softmax Regression, such as the \"lbfgs\" solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = iris[\"target\"]\n",
    "\n",
    "softmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10, random_state=42)\n",
    "softmax_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the next time you find an iris with 5 cm long and 2 cm wide petals, you can ask\n",
    "your model to tell you what type of iris it is, and it will answer Iris-Virginica (class 2)\n",
    "with 94.2% probability (or Iris-Versicolor with 5.8% probability):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_reg.predict([[5, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_reg.predict_proba([[5, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure below shows the resulting decision boundaries, represented by the background\n",
    "colors. Notice that the decision boundaries between any two classes are linear. The\n",
    "figure also shows the probabilities for the Iris-Versicolor class, represented by the\n",
    "curved lines (e.g., the line labeled with 0.450 represents the 0.45 probability boundary).\n",
    "Notice that the model can predict a class that has an estimated probability below\n",
    "0.50. For example, at the point where all decision boundaries meet, all classes have an\n",
    "equal estimated probability of 0.33."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, x1 = np.meshgrid(\n",
    "        np.linspace(0, 8, 500).reshape(-1, 1),\n",
    "        np.linspace(0, 3.5, 200).reshape(-1, 1),\n",
    "    )\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "\n",
    "\n",
    "y_proba = softmax_reg.predict_proba(X_new)\n",
    "y_predict = softmax_reg.predict(X_new)\n",
    "\n",
    "zz1 = y_proba[:, 1].reshape(x0.shape)\n",
    "zz = y_predict.reshape(x0.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X[y==2, 0], X[y==2, 1], \"g^\", label=\"Iris-Virginica\")\n",
    "plt.plot(X[y==1, 0], X[y==1, 1], \"bs\", label=\"Iris-Versicolor\")\n",
    "plt.plot(X[y==0, 0], X[y==0, 1], \"yo\", label=\"Iris-Setosa\")\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "\n",
    "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
    "contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)\n",
    "plt.clabel(contour, inline=1, fontsize=12)\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(loc=\"center left\", fontsize=14)\n",
    "plt.axis([0, 7, 0, 3.5])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice question\n",
    "\n",
    "Load the Breast cancer wisconsin (diagnostic) dataset from sklearn with\n",
    "\n",
    "bc=datasets.load_breast_cancer() \n",
    "(like we used load_iris()).\n",
    "\n",
    "\n",
    "Check the details of the dataset with bc.DESCR and bc.keys.\n",
    "\n",
    "It’s important to note that all of Scikit-Learn datasets are divided into data and target. \n",
    "data represents the features, which are the variables that help the model learn how to predict.\n",
    "target includes the actual labels. \n",
    "In our case, the target data is one column classifies the tumor as either 0 indicating malignant or 1 for benign.\n",
    "You can convert the data into panda dataframe as: (do not forget to import panadas as pd)\n",
    "\n",
    "df = pd.DataFrame(bc.data, columns=bc.feature_names)\n",
    "\n",
    "First build the logistic regression model with all the instances, and then predict the class of the same instances you used in training.\n",
    "You can check how many of each class did the model predict well with\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(true_label, predicted_label)\n",
    "\n",
    "pd.DataFrame(cm)\n",
    "\n",
    "Then you can take out some data before training with\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(bc.data,bc.target,train_size=.75, random_state=42)\n",
    "\n",
    "You can use the X_train and Y_train to train the model, then predict on X_test and compare predictions with Y_test.\n",
    "Is there a difference in performance?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mlcourse]",
   "language": "python",
   "name": "conda-env-mlcourse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
